\chapter{Introduction}
\label{ch:p1:introduction}

The goal of lattice Quantum Chromodynamics (lattice QCD) is to numerically simulate the interactions of quarks and gluons on a finite and discrete Euclidean spacetime grid, enabling first-principles predictions of hadron properties. 

Computing hadronic observables to subpercent precision, as needed for example to interpret the most recent Muon g-2 measurements~\cite{Muong-2:2023cdq,Gerardin:2020gpp}, poses formidable challenges. It requires simulating four dynamical quark flavors, incorporating quantum electrodynamics (QED) effects, using fine lattice spacings for reasonable continuum extrapolation, using physical quark masses, and high statistics. Meeting these demands substantially increases computational requirements for generating Monte Carlo samples of gauge field configurations on large lattices.
% ~footnote{See, for example, Ref.~\cite{Gerardin:2020gpp} for a more detailed picture of the complexities involved in computing hadronic contributions to $(g-2)_\mu$ to subpercent precision using lattice QCD.}.

The growing adoption of GPU-accelerated supercomputing in major facilities, largely driven by machine learning needs, presents new opportunities for using GPUs for lattice QCD codes. As lattice simulations are constrained by memory bandwidth rather than floating point operations, the high-memory throughput of GPUs is expected to give a speed-up for calculations.

Our code, openQxD-1.1~\cite{openqxd}, extends the widely used openQCD-1.6 package~\cite{openqcd}, which supports the $O(a)-$improved Wilson fermion discretization (from now on, we will refer to these codebases simply as openQxD and openQCD, respectively). openQxD extends openQCD by allowing the simulation of QCD together with quantum electrodynamics (QED). This is accomplished through the use of C$^\star$ spatial boundary conditions \cite{Kronfeld1991}, which respect locality and Gauss's law at the same time reducing finite-size effects compared to periodic boundaries or other lattice QED formulations. These features make openQxD a versatile tool for achieving higher levels of precision. The codebase is designed for efficient parallel execution on CPU-based supercomputing architectures.

openQxD uses the Hybrid Monte Carlo algorithm for dynamical fermion lattice simulations, where the most time-consuming task is the repeated inversion of a large sparse operator to solve the Dirac equation. This Dirac operator, which connects nearest neighbor points on a 4D Euclidean spacetime lattice, has indices for Dirac spin, color charge, and space-time. By porting the Dirac solver to GPUs, the overall execution time of openQxD is expected to be significantly reduced.

Among the various possibilities for GPU-accelerating lattice QCD calculations, we choose to interface openQxD with the QUDA library~\cite{QUDApaper}, which provides optimized algorithms for NVIDIA and AMD GPUs, including an efficient iterative solver for the Dirac equation. 
We implement C$^\star$ boundary conditions and QED effects, as these are features that are particular to openQxD and do not yet exist in QUDA. In this way we combine openQxD's comprehensive physics toolset with QUDA's GPU-accelerated operations, thus enhancing openQxD simulations with GPU performance.

The remainder of this paper is structured as follows: \cref{ch:p1:openqxd} introduces the openQxD codebase, while \cref{ch:p1:quda} describes the QUDA library. \cref{ch:p1:interface} details the process of interfacing the openQxD application with the QUDA library: it discusses the modifications to the memory layout of lattice fields, the implementation of $C^\star$ boundary conditions, and the extension of QUDA to handle QCD+QED simulations. \cref{ch:p1:solver} describes the steps needed to call QUDA solvers from openQxD, that is initialization, parameter setup, error handling, and finalization. \cref{ch:p1:building} explains how to build openQxD with QUDA. \cref{ch:p1:performance} includes benchmark tests of CPU and GPU implementations of the Dirac operator and solvers. The strategy for continuous integration and continuous delivery (CI/CD) is discussed in \cref{ch:p1:cicd}. Finally, \cref{ch:p1:memory} proposes a way to continue the developments regarding the continuation of the PASC project when considering HMC on GPUs.
