\chapter{Concluding remarks of part I}
\label{ch:p1:conclusions}

\dictum[Famous code comment by Paul DiLascia]{%
  If this code works, it was written by Paul DiLascia. If not, I don't know who wrote it. }%
\vskip 1em

\readit{2}

% \worktodo{
% Conclusions
% * reflect on Goals and what was achieved
% * emphasize impact
% * limitations, challenges, Lessons Learned 
% * transition to next part
% * synthesis, no restatement
% * 1-2 pages
% * lays groundwork 
% }

\tldr{lay groundwork, offloading heavy lifting}
The main goal of this part of the thesis was to enable the \RCstar collaboration to run the observable evaluation part of non-perturbative QCD+QED simulations on GPU-accelerated computing centers.
%Offloading of the heavy-lifting parts -- repeated solves of the Dirac equation -- was the focus of the developments in particular, since it represents the most expensive kernel of lattice simulations.
Offloading repeated solves of the Dirac equation was the focus of the developments in particular, since it represents the most expensive kernel of lattice simulations.
This work marks by no means the end of the story; the work merely lays crucial foundation for ongoing software developments.

\tldr{enabled more funding, compute allocations}
As a direct outcome, this work enabled the \RCstar collaboration to acquire substantial compute allocations at leading supercomputing facilities, advancing the collaboration's physics programme and securing a second round of funding for further code development~\cite{online:pasc2025}.
This enables the collaboration to continue exploring isospin-breaking corrections to hadronic observables with \Cstar boundary conditions.
%\worktodo{Marina: I would rather focus on the physics/implementation and algorithmic part, "for exploring isospin breaking corrections to hadronic observables with Cstar BCs"}
Future work, including support for configuration generation and unified field treatment (\cref{ch:p1:memory}), builds directly upon the developments established here.
%However, it made it possible to acquire large compute budgets at super-computing centers advancing the collaborations physics programme to next step and secured a second round of funding for future developments concerning configuration generation and a unified field treatment as outlined in \cref{ch:p1:memory}.

\tldr{other collabs can benefit too}
Since \openqxd is based in \openqcd, which is broadly used by the lattice community, the code developments are directly applicable to \openqcd too.
With minimal additional effort a large part of the lattice community can immediately benefit from the developments made in this thesis.

\tldr{cicd, easy integration}
The \openqxd package now interfaces seamlessly with solver kernels provided by \quda, offering flexible and abstracted integration that reduces the complexity for application-level developers.
%The software package \openqxd is now able to interface all solver kernels from \quda in versatile forms, where implementation details are abstracted away for the programmer of \openqxd.
Existing tools can benefit from the improvements with low porting effort.
Code robustness, trust and stability was improved by introducing an automated CI/CD pipeline.

\tldr{scientific software is hard to do right}
Writing scientific software is challenging and can be as frustrating as it is rewarding.
%Every concept we know from industry software engineering is harder when considering scientific software development and needs more work: testing, readability, maintainability, complexity, scalability to just name a few.
Practices common in industry -- such as modular design, code readability, testing, deployment, and scalability -- require significantly more effort to achieve in the context of scientific computing.
However, that does not temper the satisfaction one feels when a piece of code finally works or a tenfold speedup was achieved.

\tldr{testing is pain, on real-world problems}
Testing scientific software is not straightforward.
One lesson learned was that testing should reflect production use cases ideally running on the target system using target parameters and target problem sizes, as opposed to a single local workstation on toy problems.
Due to its parallel nature and utilization of multi-threading, multi-processing, CPUs, GPUs and being cutting edge in hardware and software advancements, systematic testing of scientific software is more essential than ever.
This is particularly true when we compute observables for which no independent validation exists yet.
In such cases, trust in the software must be earned through rigorous and reproducible validation workflows.

\tldr{summary of interface, challenges overcome}
The interface allows accessing solvers in various ways; directly via \openqxd mechanisms suitable for application-level developers, directly after initialization simplifying new programs that only use the GPU solver, asynchronously ideal for heterogeneous workloads where CPU work and GPU solves overlap.
Programs alternating or overlapping CPU workloads with GPU solves may lead to imbalanced scaling due to their differing performance characteristics.
The dual grid approach resolves this by enabling strong scaling of the CPU part separate to the GPU part.
The above mentioned features can be combined and all of them are capable of solving for multiple right-hand sides improving the energy footprint and time to solution.

\tldr{summary performance data}
The performance data shows that the GH200 node can efficiently be utilized.
The excellent weak scaling indicates readiness of the code for larger problem sizes.
As discussed in the last paragraph, many mechanisms exist to help mitigate strong scaling challenges, especially those inherent to multigrid solvers.
While the dual grid performance impact is negligible, CPU-GPU data transfer overheads remain a concern.
We proposed a field memory management system aiming at these overheads and reducing node internal CPU-GPU interconnect pressure.

\tldr{transition to part 2}
In the next part, we will approach the problem of efficient utilization of computational resources from an orthogonal perspective: through algorithmic development informed by physical insight.
The goal is to improve the estimation of physical observables not merely by accelerating code but by enhancing the efficiency of the simulations themselves via theoretical and algorithmic method development.
%It will deal with algorithmic developments backed by physical insight to obtain better estimates of physical observables.


