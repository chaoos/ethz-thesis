\chapter{Low-mode averaging}
\label{ch:p2:lma}

\readit{0}

\worktodo{
* Separate chapter LMA
    * 4p from intro
    * spectral prop from 2pt
    * V2 problem
    * X-term problem
}


\tldr{LMA original and usages}
This chapter introduces the variance reduction method of low-mode averaging~\cite{Neff_2001,DeGrand_2004,Giusti_2004} (LMA) and some of its many variants which had a large influence in this thesis.
The method consists of decomposing the quark propagator $D^{-1}$ into a truncated spectral decomposition and a remainder, defined precisely in the following.
%\worktodo{Tim: Defined precisely below}
The spectral sum can be easily volume averaged pushing its variance to its gauge noise.
This method is used widely in the field for quantities suffering from the signal-to-noise ratio problem in the long distance.
Examples are the HVP~\cite{bmw_2024,Kuberski_2023,Aubin:2022hgm,Bazavov:2024eou,RBC_2024} and HLbL~\cite{Lin:2024khg} contributions to the muon $g-2$ or derived quantities~\cite{ExtendedTwistedMass:2025tpc}, baryon three-point functions~\cite{Yang:2015zja,Ohki:2012jyg} and masses \cite{Bali:2010se}, nucleon charges~\cite{Yamanaka:2018uud}, reweighting of the RHMC~\cite{Kuberski:2023zky}, decay constants~\cite{Bali:2014pva}, low energy couplings~\cite{Bernardoni:2011kd}, form factors~\cite{JLQCD:2009ofg} and many more.
Clearly, the method is only beneficial if the volume average over the deflated subspace captures the large distance behavior of the observable and one can afford a numerical determination of the linear-in-volume-dimensional low-mode subspace.
%Clearly, the method is only beneficial if the majority of the noise \worktodo{Tim: What noise????? The noise introduced by *not* computing the translation average.} comes from the low-mode contribution and one can afford a numerical determination of the linear-in-volume scaling low-mode subspace.

\tldr{V2 problem}
The method usually comes with two tasks, both of which are computationally expensive.
First, one has to determine a certain number of eigenmodes of the Hermitian or the non-Hermitian Dirac operator.
This number heavily depends on the lattice volume, the physical quantity of interest and the variance reduction goal.
%Obviously the more modes involved, the better the outcome.
% The number of low modes required for constant variance reduction is proportional to the lattice volume, because the size of the low-mode subspace is proportional to the lattice volume~\cite{banks1980}.
% Therefore, the computational work to determine the low-mode subspace scales at least quadratic in the lattice volume, which we will refer to as \emph{$V^{2}$-problem}~\cite{Luescher2007}.
% This fact makes LMA on large-scale physical point lattices prohibitively expensive.
%On recent high-precision determinations of the HVP contribution to the muon $g-2$ this number is driven to the limit of affordability; \numrange{1000}{6000}~\cite{Djukanovic:2024cmq,RBC_2024,bmw_2024,Aubin:2022hgm} with varying satisfaction.

\tldr{X-term problem}
Second, when plugging the decomposition of the propagator into two-point correlators, we obtain a cross-term mixing low- and high-mode contributions, usually called \emph{rest-eigen} term.
If the number of low modes is not sufficiently high\footnote{The numbers mentioned in the previous paragraph are \emph{not} high enough.}, the cross-term still contributes non-negligibly to the large distance variance -- an issue which we will refer to as \emph{cross-term problem}.
The cost of evaluating this term is proportional to the number of low modes.

Many extensions of LMA have been proposed mainly to deal with the cross-term problem.
All-mode averaging (AMA) employs many cheap low-precision solves, corrected with a few high-precision solves~\cite{Blum_2012,CAA,RBC_2018,Blum_2015}.
The truncated solver method (TSM) very similarly uses many heavily truncated solves, corrected with a few high-precision solves~\cite{bmw_2017,Kuberski:2023zky}.
In refs.~\cite{fermi_2023,lynch2023} the cross-term together with the high-mode (rest-rest) term is evaluated stochastically.

\section{Spectral decomposition}

LMA introduces an improved estimator for \cref{eq:G:corr:connected} using the spectral decomposition of the Hermitian Dirac operator,
\begin{equation}
Q = \sum_{n=0}^{12V-1} \lambda_{n} \evec_{n} \evec_{n}^{\dagger} \;,
\qquad
Q = \gamma^{5} D \;,
\qquad
Q^{\dagger} = Q \;,
\end{equation}
into an orthonormal eigenbasis $\mathcal{B} = \{ \evec_n \}_{n=0}^{12V-1}$ with
\begin{equation}
Q \evec_{n} = \lambda_{n} \evec_{n} \;,
\qquad
\evec_{n}^{\dagger} \evec_{m} = \delta_{nm} \;,
\end{equation}
with real eigenvalues $\lambda_{n} \in \mathbb{R}$.
The same decomposition holds for the propagator
\begin{equation} \label{eq:prop:all-to-all}
\propxy{x}{y} = \sum_{n=0}^{12V-1} \frac{1}{\lambda_{n}} \fieldx{\evec_{n}}{x} \fieldx{\evec_{n}^{\dagger}}{y} \gamma^{5} \;,
\end{equation}
where the $\gamma^{5}$ at the end is because $S = Q^{-1} \gamma^{5}$.
Analogously, we emphasize just as \cref{eq:prop:pt-to-all} is an exact point-to-all propagator (no approximation involved), the above is an exact all-to-all propagator.

In its current state, this propagator is not appropriate for numerical computations, because the full eigen-decomposition into all $12V$ modes is prohibitively expensive.
Although we can truncate the sum in \cref{eq:prop:all-to-all} and only take the $\Nc \ll 12 V$ modes with lowest magnitude eigenvalues, \ie the largest $\frac{1}{\lambda_{n}}$.
These should be the modes that contribute most to the propagator.
To do this, we define an orthogonal projector to the space of the $\Nc$ lowest modes as
\begin{equation} \label{eq:lma:projector}
P = \sum_{n=0}^{\Nc-1} \evec_n \evec_n^{\dagger} \;,
\qquad
P^{2} = P = P^{\dagger} \;.
\end{equation}
The full all-to-all propagator is then decomposed into two terms
\begin{equation} \label{eq:prop:lma}
S
= \underbrace{\sum_{n=0}^{\Nc-1}
  \frac{1}{\lambda_{n}} \evec_{n} \evec_{n}^{\dagger} \gamma^{5}}_{=\prop_e}
+ \underbrace{(1-P) Q^{-1} \gamma^{5}}_{=\prop_r} \;,
\end{equation}
one contribution along the low eigenmode space $\prop_e$ and a remainder $\prop_r$.
We note that this decomposition is an exact equality.

The two-point connected correlator \cref{eq:G:corr:connected} has two slots for propagators.
We can either plug \cref{eq:prop:lma} in only one slot or in both of them.
Both options result in different valid estimators.
We will continue with the latter and since the propagator is now a sum of two terms, we obtain four terms,
\begin{equation}
G^{\text{conn}}(t)
= G_{ee}^{\text{conn}}(t)
+ G_{er}^{\text{conn}}(t)
+ G_{re}^{\text{conn}}(t)
+ G_{rr}^{\text{conn}}(t) \;.
% - \frac{a^{3}}{3 \lvert \Omega \rvert}
% \sum_{y \in \Omega}
% \sum_{\vect{x}} \;
% \sum_{k=1}^{3}
% \tr \left[
%   \prop(t+y_0, \vect{x}|y) \gamma_k \prop(y|t+y_0, \vect{x}) \gamma_k
% \right].
\end{equation}
The individual terms can be worked out using properties of the $\gamma$-matrices
\begin{align}
\{\gamma^{5}, \gamma_{\mu}\} &= 0 \;,        &  (\gamma^{5})^{2} &= \id \;, \\
\gamma_{\mu}^{\dagger} &= \gamma_{\mu} \;,   &  (\gamma^{5})^{\dagger} &= \gamma^{5} \;.
\end{align}
Since their evaluations differ quite substantially, we will individually discuss them in the following.

\section{The rest-rest term}

The rest-rest term is the simplest one in the decomposition.
It is just the standard form \cref{eq:G:corr:connected}, but with $S$ being replaced by the projected $(1-P)S$.
If enough low modes were deflated, this term does not significantly contribute to the variance in the large distance.
It can then be estimated using standard estimators, for instance point- or stochastic time-diluted sources as introduced in \cref{sec:point:prop,sec:stochastic:prop}.

\section{The eigen-eigen term}

For the eigen-eigen term we find
\begin{multline} \label{eq:Gconn:eigen-eigen}
G_{ee}^{\text{conn}}(t)
% = - \frac{a^{3}}{3 V}
% \sum_{k=1}^{3}
% \sum_{n,m=0}^{\Nc-1} \frac{1}{\lambda_{n} \lambda_{m}}
% \sum_{y_0=0}^{L_0-1} \;
% \sprod{
%   \evec_{n}^{(y_0)}
% }{
%   \gamma^{5}
%   \gamma_k
%   \evec_{m}^{(y_0)}
% } \\
% \cdot
% \sprod{
%   \evec_{m}^{(t+y_0)}
% }{
%   \gamma^{5}
%   \gamma_k
%   \evec_{n}^{(t+y_0)}
% }.
= - \frac{a^{3}}{3 V}
\sum_{k=1}^{3}
\sum_{n,m=0}^{\Nc-1} \frac{1}{\lambda_{n} \lambda_{m}}
\sum_{\vect{x},y} \;
\sprod{
  \fieldx{\evec_{n}}{y}
}{
  \gamma^{5}
  \gamma_k
  \fieldx{\evec_{m}}{y}
} \\
\cdot
\sprod{
  \fieldx{\evec_{m}}{t+y_0, \vect{x}}
}{
  \gamma^{5}
  \gamma_k
  \fieldx{\evec_{n}}{t+y_0, \vect{x}}
} \;,
\end{multline}
where the $\sprod{\cdot}{\cdot}$ denotes the standard spinor product.
Since the eigenmodes are defined on every lattice point, we can easily perform a full volume average of this contribution, $\lvert \Omega \rvert = V$.
This is characteristic to this type of estimator but also crucial, because now the eigen-eigen term is at the minimal variance (its gauge noise level) and its variance cannot be lowered further for a fixed gauge config\footnote{Without employing multi-level sampling schemes.}.
Due to the exact averaging over the subspace of low modes, this class of estimators is known under the name low-mode \emph{averaging}.

\section{The cross-term}

The two crossing terms, rest-eigen and eigen-rest, show some symmetry,
\begin{align}
G_{er}^{\text{conn}}(t) &=
\frac{a^{3}}{3 L^{3} \lvert \mathcal{T} \rvert}
\sum_{y_0 \in \mathcal{T}} \;
\sum_{n=0}^{\Nc-1} \; \frac{1}{\lambda_{n}}
\sum_{k=1}^{3} \;
\sprod{
  (1-P) S
  \gamma_k
  \evec_{n}^{\tslice{y_0}}
}{
  \gamma^{5}
  \gamma_k
  \evec_{n}^{\tslice{t+y_0}}
} \;, \\
G_{re}^{\text{conn}}(t) &=
\frac{a^{3}}{3 L^{3} \lvert \mathcal{T} \rvert}
\sum_{y_0 \in \mathcal{T}} \;
\sum_{n=0}^{\Nc-1} \frac{1}{\lambda_{n}} \;
\sum_{k=1}^{3} \;
\sprod{
  \gamma^{5}
  \gamma_k
  \evec_{n}^{\tslice{t+y_0}}
}{
  (1-P) S
  \gamma_k
  \evec_{n}^{\tslice{y_0}}
} \;,
\end{align}
where we have introduced the time-diluted modes
\begin{equation}
\fieldx{\evec_{n}^{\tslice{t}}}{x} =
\begin{cases}
  \fieldx{\evec_{n}}{x} & \text{if } x_0 = t \;, \\
  0 & \text{otherwise} \;.
\end{cases} 
\end{equation}
They were defined such that the original mode can be recovered by summing over all times and they are mutually orthogonal but not orthonormal anymore
\begin{equation}
\evec_{n} = \sum_{t=0}^{L_0-1} \evec_{n}^{\tslice{t}} \;,
\qquad
\sprod{\evec_{n}^{\tslice{t}}}{\evec_{m}^{\tslice{s}}} \sim \delta_{nm} \delta_{ts} \;.
\end{equation}
Both cross-terms are in general complex valued, but they are complex conjugates of each other and thus we only need to estimate one of them
\begin{align}
G_{\cross}^{\text{conn}}(t)
&= G_{er}^{\text{conn}}(t) + G_{re}^{\text{conn}}(t) \\
&= 2 \Re{G_{er}^{\text{conn}}(t)} \;.
\end{align}

Clearly the cross-term is expensive to calculate exactly, since we need to solve the Dirac equation on every time-diluted mode for every $\gamma$-matrix we want to evaluate the correlator for, \ie solve the systems
\begin{equation}
D \psi = (1-P) \gamma_k \evec_{n}^{\tslice{t}}
\end{equation}
for all $k = 1,2,3$, $t \in \stime$ and $n = 0, \ldots, \Nc-1$.
This makes $3 \Nc L_0$ solves!
Many improved estimators will not include the whole time extend, all $\gamma$-matrices or all modes but a subset of them.
Additionally, many LMA-scenarios employ heavily truncated solves on subsets for the above and correct the bias regularly with differences of high precision and truncated solves to obtain an unbiased estimator~\cite{bmw_2017,Kuberski_2023}.
Alternatively one can introduce a stochastic estimator for the cross-term~\cite{lynch2023,fermi_2023}.
However, the problem related to the evaluation of the cross-term, we will call the \emph{cross-term problem}.
%We called this the \emph{cross-term problem}.

Another noteworthy variant goes under the name of all-mode averaging (AMA)~\cite{Blum_2012,Blum_2015,CAA,RBC_2018}.
It has a slightly different propagator decomposition
\begin{equation}
\prop
= \underbrace{\prop - \prop_{\text{AMA}}}_{\prop_{\text{r}}}
+ \underbrace{\sum_{n=0}^{\Nc-1} \frac{1}{\lambda_n} \evec_n \evec_n^{\dagger} \gamma^{5}
+ P_n(Q) P \gamma^{5}}_{\prop_{\text{AMA}}} \;.
\end{equation}
Here $P_n(Q)$ is a polynomial in $Q$ usually obtained from an implicitly generated polynomial of a truncated solve (TSM)~\cite{Bali_2009} preconditioned by the eigenmodes and $P$ is defined as in \cref{eq:lma:projector}.
Here, the TSM does not need bias correction, since the above is a exact decomposition, irrespective of the truncation scheme.
For this estimator, it is also possible to set $N_c=0$ \cite{Blum_2012}.
The terms involving $\prop_{r}$ are treated with very few sources, since they do not contribute much to the overall variance.
Nevertheless, AMA merely moves the problem of the cross-term into the term including $\prop_{\text{AMA}}$ without solving it at its root.
The polynomial's purpose is to extend the averaging from low modes to all modes, hence the name \emph{all}-mode averaging.
However, in all scenarios the cost of this term poses one of the major limiters of LMA, a problem which we aim to solve in the remainder of this part.

\section{Summary}
\label{sec:lma:summary}

\worktodo{summary}
Clearly the cost and the effectiveness of LMA and variants critically depend on the number of low modes $\Nc$ where the spectral sum is truncated.
Due to Banks and Casher\cite{banks1980}, $\Nc$ need to be proportional to the lattice volume, a condition hard to satisfy on large lattices.
We called this the \emph{$V^{2}$-problem}.
%Low-mode averaging as state-of-the-art method for this class of problems was introduced and its main limiters were stated in form of the $V^{2}$- and the cross-term problem.
%\tldr{V2 vs Xterm vs cost}
We observe a non-trivial interplay of computational cost and variance reduction.
In order to achieve a fair variance reduction we want to drive the number of deflated low modes as high as we can afford.
On the other hand, the cost of the eigensolver and of the cross-term increases proportional.
It is not feasible anymore on large lattices to drive the number of eigenmodes so high to suppress the variance on the cross-term such that its evaluation is cheap.
An often unmentioned issue is the immense memory footprint of $\bigO(1000)$ low modes that have to be in memory all at the same time, sometimes even driving the required node count of the job into the bad strong scaling region.

